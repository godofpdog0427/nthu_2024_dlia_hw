{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0--eEwAS5KQ_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import typing as ty\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 777\n",
        "\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "zQEeaX0JHMhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('nvda.us.txt')\n",
        "df"
      ],
      "metadata": {
        "id": "A0N4C-Zj6bFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "\n",
        "def create_sequences(input_data, output_data, window_size, step):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(0, len(input_data) - window_size, step):\n",
        "        sequences.append(input_data[i:(i + window_size)])\n",
        "        labels.append(output_data[i + window_size])\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "\n",
        "def prepare_data_loaders(\n",
        "    raw_data: pd.DataFrame,\n",
        "    use_feature_names: ty.List[str],\n",
        "    batch_size: int = 32,\n",
        "    window_size: int = 10,\n",
        "    step: int = 15,\n",
        "    normalization: str = None) -> ty.Tuple:\n",
        "\n",
        "    # Select features\n",
        "    features = df[use_feature_names]\n",
        "    labels = df['High'].shift(-1)  # Next day's high price as label\n",
        "\n",
        "    X, y = create_sequences(features, labels, window_size=10, step=15)\n",
        "\n",
        "    print(f'Shape of data X: {X.shape}')\n",
        "    print(f'Shape of data y: {y.shape}')\n",
        "\n",
        "    # split the hold-out tests\n",
        "    ind = np.linspace(0, len(X)-1, num=int(len(X)*0.1), dtype=int) # 10% hold-out\n",
        "    x_test = X[ind]\n",
        "    y_test = y[ind]\n",
        "    all_ind = np.arange(len(X))\n",
        "    remains_ind = np.delete(all_ind, ind)\n",
        "\n",
        "    X = X[remains_ind]\n",
        "    y = y[remains_ind]\n",
        "\n",
        "    # shuffle dataset\n",
        "    ind = np.random.permutation(len(X))\n",
        "    X = X[ind]\n",
        "    y = y[ind]\n",
        "    split_point = int(X.shape[0]*0.8)\n",
        "\n",
        "    x_train = X[:split_point]\n",
        "    y_train = y[:split_point]\n",
        "    x_val = X[split_point:]\n",
        "    y_val = y[split_point:]\n",
        "\n",
        "    print(f'Shape of data x_train: {x_train.shape}')\n",
        "    print(f'Shape of data y_train: {y_train.shape}')\n",
        "    print(f'Shape of data x_val: {x_val.shape}')\n",
        "    print(f'Shape of data y_val: {y_val.shape}')\n",
        "    print(f'Shape of data x_test: {x_test.shape}')\n",
        "    print(f'Shape of data y_test: {y_test.shape}')\n",
        "\n",
        "    # Apply normalization\n",
        "    # Labels (y)\n",
        "    y_scaler = StandardScaler()\n",
        "    y_train_normalized = y_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1)\n",
        "    y_val_normalized = y_scaler.transform(y_val.reshape(-1, 1)).reshape(-1)\n",
        "    y_test_normalized = y_scaler.transform(y_test.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "    # Features (X)\n",
        "    if normalization is not None:\n",
        "        assert normalization in ('minmax', 'std')\n",
        "\n",
        "        for c in range(X.shape[2]):\n",
        "            scaler = MinMaxScaler() if normalization == 'minmax' else StandardScaler()\n",
        "            x_train[:, :, c] = scaler.fit_transform(x_train[:, :, c].reshape(-1, 1)).reshape(-1, window_size)\n",
        "            x_val[:, :, c] = scaler.transform(x_val[:, :, c].reshape(-1, 1)).reshape(-1, window_size)\n",
        "            x_test[:, :, c] = scaler.transform(x_test[:, :, c].reshape(-1, 1)).reshape(-1, window_size)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    x_train = torch.from_numpy(x_train).float()\n",
        "    y_train = torch.from_numpy(y_train).float()\n",
        "    y_train_normalized = torch.from_numpy(y_train_normalized).float()\n",
        "\n",
        "    x_val = torch.from_numpy(x_val).float()\n",
        "    y_val = torch.from_numpy(y_val).float()\n",
        "    y_val_normalized = torch.from_numpy(y_val_normalized).float()\n",
        "\n",
        "    x_test = torch.from_numpy(x_test).float()\n",
        "    y_test = torch.from_numpy(y_test).float()\n",
        "    y_test_normalized = torch.from_numpy(y_test_normalized).float()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TensorDataset(x_train, y_train, y_train_normalized)\n",
        "    val_dataset = TensorDataset(x_val, y_val, y_val_normalized)\n",
        "    test_dataset = TensorDataset(x_test, y_test, y_test_normalized)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f'Number of samples in training and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')\n",
        "\n",
        "    return (train_loader, val_loader, test_loader, y_scaler)\n"
      ],
      "metadata": {
        "id": "SB27O-8Q5VWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int, output_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "        model: nn.Module,\n",
        "        loader: DataLoader,\n",
        "        criterion: ty.Callable,\n",
        "        optimizer: optim.Optimizer,\n",
        "        device: str) -> float:\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for features, _, labels in loader:\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(features).squeeze(-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(loader)\n",
        "\n",
        "    return avg_train_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: ty.Callable,\n",
        "    y_scaler: StandardScaler,\n",
        "    device: str) -> float:\n",
        "\n",
        "    model.eval()\n",
        "    pred_value = []\n",
        "    actual_value = []\n",
        "\n",
        "    for features, labels, _ in loader:\n",
        "            features = features.to(device)\n",
        "            outputs = model(features).squeeze(-1).cpu().numpy()\n",
        "            outputs = y_scaler.inverse_transform(outputs.reshape(-1, 1)).reshape(-1)\n",
        "            pred_value.append(torch.from_numpy(outputs))\n",
        "            actual_value.append(labels)\n",
        "\n",
        "    pred_value = torch.cat(pred_value)\n",
        "    actual_value = torch.cat(actual_value)\n",
        "    eval_loss = criterion(pred_value, actual_value)\n",
        "\n",
        "    return eval_loss.item()\n",
        "\n",
        "\n",
        "def run_experiment(\n",
        "    epochs: int,\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    test_loader: DataLoader,\n",
        "    y_scaler: StandardScaler,\n",
        "    device: str) -> ty.Dict[str, ty.List[float]]:\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0)\n",
        "\n",
        "    metrics = defaultdict(list)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # Training\n",
        "        _ = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        curr_train_loss = eval_one_epoch(model, train_loader, criterion, y_scaler, device)\n",
        "\n",
        "        # Validation\n",
        "        curr_val_loss = eval_one_epoch(model, val_loader, criterion, y_scaler, device)\n",
        "\n",
        "        # Checkpoint\n",
        "        if curr_val_loss < best_val_loss:\n",
        "            best_val_loss = curr_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train loss: {curr_train_loss:.4f}, Val loss: {curr_val_loss:.4f}, Best Val loss: {best_val_loss:.4f}')\n",
        "\n",
        "        metrics['train_losses'].append(curr_train_loss)\n",
        "        metrics['val_losses'].append(curr_val_loss)\n",
        "\n",
        "\n",
        "    # Testing\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "    test_loss = eval_one_epoch(model, test_loader, criterion, y_scaler, device)\n",
        "    print(f'test_loss : {test_loss}')\n",
        "\n",
        "    metrics['test_loss'].append(test_loss)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def find_best_scores(training_losses, validation_losses):\n",
        "    # Find the index of the best (minimum) validation score\n",
        "    best_index = validation_losses.index(min(validation_losses))\n",
        "\n",
        "    # Retrieve the corresponding training and testing scores\n",
        "    best_training_score = training_losses[best_index]\n",
        "    best_validation_score = validation_losses[best_index]\n",
        "\n",
        "    return best_training_score, best_validation_score"
      ],
      "metadata": {
        "id": "-Ov0JaXM8CXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1\n",
        "Train the model using 3 different combinations of window size and step"
      ],
      "metadata": {
        "id": "LhCQrqxICDFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_FEATURE_NAMES = ['Open', 'High', 'Low', 'Close']\n",
        "HIDDEN_DIM = 500\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 32\n",
        "WINDOW_SIZE = 10\n",
        "STEP = 15\n",
        "NUM_EPOCHS = 100\n",
        "DEVICE = 'cpu'\n",
        "\n",
        "\n",
        "EXP_COMBINATIONS = [\n",
        "    {'window_size': 10, 'step': 30},\n",
        "    {'window_size': 10, 'step': 10},\n",
        "    {'window_size': 30, 'step': 15},\n",
        "    {'window_size': 20, 'step': 30},\n",
        "    {'window_size': 30, 'step': 30}\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "model = LSTMModel(\n",
        "    input_dim=len(USE_FEATURE_NAMES),\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_dim=1).to(DEVICE)\n",
        "\n",
        "\n",
        "results_dict = defaultdict(list)\n",
        "\n",
        "for combination in EXP_COMBINATIONS:\n",
        "    print('combination :')\n",
        "    print(combination)\n",
        "    window_size = combination['window_size']\n",
        "    step = combination['step']\n",
        "\n",
        "    train_loader, val_loader, test_loader, y_scaler = \\\n",
        "        prepare_data_loaders(df, USE_FEATURE_NAMES, BATCH_SIZE, window_size, step)\n",
        "\n",
        "    metrics = run_experiment(NUM_EPOCHS, model, train_loader, val_loader, test_loader, y_scaler, DEVICE)\n",
        "\n",
        "    train_loss, val_loss = find_best_scores(metrics['train_losses'], metrics['val_losses'])\n",
        "\n",
        "    results_dict['window_size'].append(window_size)\n",
        "    results_dict['step'].append(step)\n",
        "    results_dict['train_mse'].append(train_loss)\n",
        "    results_dict['val_mse'].append(val_loss)\n",
        "    results_dict['test_mse'].append(metrics['test_loss'][0])\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results_dict)\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "xsn6YiLf-_ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2-1\n",
        "Include 'Volume' as an additional input feature in your model.\n"
      ],
      "metadata": {
        "id": "gi0VZ_jHMRhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_FEATURE_NAMES = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "HIDDEN_DIM = 500\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 32\n",
        "WINDOW_SIZE = 10\n",
        "STEP = 15\n",
        "NUM_EPOCHS = 100\n",
        "# DEVICE = 'cpu'\n",
        "\n",
        "# four features\n",
        "model = LSTMModel(\n",
        "    input_dim=len(USE_FEATURE_NAMES[:4]),\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_dim=1).to(DEVICE)\n",
        "\n",
        "train_loader, val_loader, test_loader, y_scaler = \\\n",
        "        prepare_data_loaders(df, USE_FEATURE_NAMES[:4], BATCH_SIZE, WINDOW_SIZE, STEP)\n",
        "\n",
        "metrics_four_features = run_experiment(\n",
        "    NUM_EPOCHS, model,\n",
        "    train_loader, val_loader, test_loader, y_scaler, DEVICE)\n",
        "\n",
        "\n",
        "# five features\n",
        "model = LSTMModel(\n",
        "    input_dim=len(USE_FEATURE_NAMES),\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_dim=1).to(DEVICE)\n",
        "\n",
        "train_loader, val_loader, test_loader, y_scaler = \\\n",
        "        prepare_data_loaders(df, USE_FEATURE_NAMES, BATCH_SIZE, WINDOW_SIZE, STEP)\n",
        "\n",
        "metrics_five_features = run_experiment(\n",
        "    NUM_EPOCHS, model,\n",
        "    train_loader, val_loader, test_loader, y_scaler, DEVICE)\n",
        "\n",
        "\n",
        "plt.plot(metrics_four_features['train_losses'], label='train loss w/ four features')\n",
        "plt.plot(metrics_four_features['val_losses'], label='validation loss w/ four features')\n",
        "\n",
        "plt.plot(metrics_five_features['train_losses'], label='train loss w/ five features')\n",
        "plt.plot(metrics_five_features['val_losses'], label='validation loss w/ five features')\n",
        "\n",
        "plt.title('Learning curve comparison')\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'Test loss (w/ four features) : {metrics_four_features[\"test_loss\"]}')\n",
        "print(f'Test loss (w/ five features) : {metrics_five_features[\"test_loss\"]}')"
      ],
      "metadata": {
        "id": "1w3Po9fyDi5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df.Volume, bins=30)\n",
        "plt.title('Distribution of Volume')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hkWKSKejsxsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2-2\n",
        "Explore and report on the best combination of input features that yields the best MSE"
      ],
      "metadata": {
        "id": "AZwzmEbWPsQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_FEATURE_NAMES = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "HIDDEN_DIM = 500\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 32\n",
        "WINDOW_SIZE = 10\n",
        "STEP = 10\n",
        "NUM_EPOCHS = 100\n",
        "# DEVICE = 'cpu'\n",
        "\n",
        "\n",
        "USE_FEATURE_NAMES_COMBINATIONS = [\n",
        "    ['Open', 'High', 'Low', 'Close', 'Volume'],\n",
        "    ['Open', 'High', 'Low', 'Close'],\n",
        "    ['High', 'Low', 'Close', 'Volume'],\n",
        "    ['Open', 'Low', 'Close', 'Volume'],\n",
        "    ['Open', 'High', 'Close', 'Volume'],\n",
        "    ['Open', 'High', 'Low', 'Volume'],\n",
        "    ['Open'],\n",
        "    ['High'],\n",
        "    ['Low'],\n",
        "    ['Close'],\n",
        "    ['Volume']\n",
        "]\n",
        "\n",
        "results_dict = defaultdict(list)\n",
        "\n",
        "for use_feature_names in USE_FEATURE_NAMES_COMBINATIONS:\n",
        "\n",
        "    # four features\n",
        "    model = LSTMModel(\n",
        "        input_dim=len(use_feature_names),\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        output_dim=1).to(DEVICE)\n",
        "\n",
        "    train_loader, val_loader, test_loader, y_scaler = \\\n",
        "        prepare_data_loaders(df, use_feature_names, BATCH_SIZE, WINDOW_SIZE, STEP)\n",
        "\n",
        "    metrics = run_experiment(\n",
        "        NUM_EPOCHS, model,\n",
        "        train_loader, val_loader, test_loader, y_scaler, DEVICE)\n",
        "\n",
        "\n",
        "    train_loss, val_loss = find_best_scores(metrics['train_losses'], metrics['val_losses'])\n",
        "\n",
        "    for feature_name in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
        "        if feature_name in use_feature_names:\n",
        "            results_dict[feature_name].append(1)\n",
        "        else:\n",
        "            results_dict[feature_name].append(0)\n",
        "\n",
        "    results_dict['train_mse'].append(train_loss)\n",
        "    results_dict['val_mse'].append(val_loss)\n",
        "    results_dict['test_mse'].append(metrics['test_loss'][0])\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results_dict)\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "1u-gdyeUOCMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3:\n",
        "Analyze the performance of the model with and without normalized inputs in Lab 4"
      ],
      "metadata": {
        "id": "u5Q8triJFDjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_FEATURE_NAMES = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "HIDDEN_DIM = 500\n",
        "NUM_LAYERS = 2\n",
        "BATCH_SIZE = 32\n",
        "WINDOW_SIZE = 10\n",
        "STEP = 15\n",
        "NUM_EPOCHS = 100\n",
        "# DEVICE = 'cpu'\n",
        "\n",
        "# W/O Normalization\n",
        "model = LSTMModel(\n",
        "    input_dim=len(USE_FEATURE_NAMES),\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_dim=1).to(DEVICE)\n",
        "\n",
        "train_loader, val_loader, test_loader, y_scaler = \\\n",
        "        prepare_data_loaders(df, USE_FEATURE_NAMES[:4], BATCH_SIZE, WINDOW_SIZE, STEP)\n",
        "\n",
        "metrics_without_normalization = run_experiment(\n",
        "    NUM_EPOCHS, model,\n",
        "    train_loader, val_loader, test_loader, y_scaler, DEVICE)\n",
        "\n",
        "\n",
        "# Apply Minmax\n",
        "model = LSTMModel(\n",
        "    input_dim=len(USE_FEATURE_NAMES),\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_dim=1).to(DEVICE)\n",
        "\n",
        "train_loader, val_loader, test_loader, y_scaler = \\\n",
        "        prepare_data_loaders(df, USE_FEATURE_NAMES, BATCH_SIZE, WINDOW_SIZE, STEP, 'minmax')\n",
        "\n",
        "metrics_with_minmax = run_experiment(\n",
        "    NUM_EPOCHS, model,\n",
        "    train_loader, val_loader, test_loader, y_scaler, DEVICE)\n",
        "\n",
        "\n",
        "# Apply Z-score (std)\n",
        "model = LSTMModel(\n",
        "    input_dim=len(USE_FEATURE_NAMES),\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    output_dim=1).to(DEVICE)\n",
        "\n",
        "train_loader, val_loader, test_loader, y_scaler = \\\n",
        "        prepare_data_loaders(df, USE_FEATURE_NAMES, BATCH_SIZE, WINDOW_SIZE, STEP, 'std')\n",
        "\n",
        "metrics_with_std = run_experiment(\n",
        "    NUM_EPOCHS, model,\n",
        "    train_loader, val_loader, test_loader, y_scaler, DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(metrics_without_normalization['train_losses'], label='train loss w/o normalization')\n",
        "plt.plot(metrics_without_normalization['val_losses'], label='validation loss w/o normalization')\n",
        "\n",
        "plt.plot(metrics_with_minmax['train_losses'], label='train loss w/ Minmax normalization')\n",
        "plt.plot(metrics_with_minmax['val_losses'], label='validation loss w/ Minmax normalization')\n",
        "\n",
        "plt.plot(metrics_with_std['train_losses'], label='train loss w/ Z-score (std) normalization')\n",
        "plt.plot(metrics_with_std['val_losses'], label='validation loss w/ Z-score (std) normalization')\n",
        "\n",
        "plt.title('Learning curve comparison')\n",
        "plt.grid()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(f'Test loss (w/o normalization) : {metrics_without_normalization[\"test_loss\"]}')\n",
        "print(f'Test loss (w/ Minmax) : {metrics_with_minmax[\"test_loss\"]}')\n",
        "print(f'Test loss (w/ Z-scpre) : {metrics_with_std[\"test_loss\"]}')"
      ],
      "metadata": {
        "id": "Mym1OyM_ODwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wN-PyAU3IaNx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}