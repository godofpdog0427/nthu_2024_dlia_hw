{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "###### HW2\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "#### Spring 2024\n",
        "\n",
        "#### 11220IEEM 513600\n",
        "\n",
        "#### Deep Learning and Industrial Applications\n",
        "    \n",
        "## Homework 3\n",
        "</div>"
      ],
      "metadata": {
        "id": "pPvMyUhSSiSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UuaOpPTZSg9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROOT = '/content/drive/MyDrive/NTHU/Deep_Learning_HomeWork/HW3/wood'\n",
        "ROOT = 'wood'\n",
        "TRAIN_RATIO = 0.8\n",
        "SEED = 42\n",
        "DEVICE = 'cuda'"
      ],
      "metadata": {
        "id": "AtpBBRlQTU3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "source_folder = '/content/drive/MyDrive/NTHU/Deep_Learning_HomeWork/HW3/wood'\n",
        "target_folder = ROOT\n",
        "\n",
        "if not os.path.exists(target_folder):\n",
        "    shutil.copytree(source_folder, target_folder)"
      ],
      "metadata": {
        "id": "-g66Scj6Ea0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "YHN7wHwB7zHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.\n",
        "Download the MVTec Anomaly Detection Dataset from Kaggle (here). Select one type of product from the dataset. Document the following details about your dataset:"
      ],
      "metadata": {
        "id": "ac0gT8esTlMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Number of defect classes\n",
        "all_classes = os.listdir(os.path.join(ROOT, 'test'))\n",
        "print(f'Number of defect classes is {len(all_classes[1:])}')\n",
        "\n",
        "# Types of defect classes\n",
        "print(f'Types of defect classes: {all_classes[1:]}')"
      ],
      "metadata": {
        "id": "gzuf-AfOTkk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images used in your datase\n",
        "# Distribution of training and test data\n",
        "\n",
        "\n",
        "import random\n",
        "import typing as ty\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def get_image_paths_by_class(directory: str) -> ty.Dict[str, list]:\n",
        "    class_paths = defaultdict(list)\n",
        "\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                class_name = os.path.basename(root)\n",
        "                class_paths[class_name].append(os.path.join(root, file))\n",
        "    return class_paths\n",
        "\n",
        "\n",
        "def split_dataset_by_class(image_paths_by_class, train_size=0.7, validation_size=0.0, seed=SEED):\n",
        "    adjusted_train_size = train_size + validation_size\n",
        "    train_image_paths, validation_image_paths, test_image_paths = [], [], []\n",
        "    random.seed(seed)\n",
        "    for class_name, paths in image_paths_by_class.items():\n",
        "        random.shuffle(paths)\n",
        "        train_count = int(len(paths) * adjusted_train_size)\n",
        "        validation_count = int(len(paths) * validation_size)\n",
        "\n",
        "        train_validation_split = paths[:train_count]\n",
        "        validation_image_paths.extend(train_validation_split[:validation_count])\n",
        "        train_image_paths.extend(train_validation_split[validation_count:])\n",
        "        test_image_paths.extend(paths[train_count:])\n",
        "\n",
        "    return train_image_paths, validation_image_paths, test_image_paths\n",
        "\n",
        "train_dir = os.path.join(ROOT, 'test')\n",
        "\n",
        "image_paths_by_class = get_image_paths_by_class(train_dir)\n",
        "train_image_paths, validation_image_paths, test_image_paths = split_dataset_by_class(image_paths_by_class)\n",
        "\n",
        "print(f'Total images in training set: {len(train_image_paths)}')\n",
        "print(f'Total images in validation set: {len(validation_image_paths)}')\n",
        "print(f'Total images in test set: {len(test_image_paths)}')\n"
      ],
      "metadata": {
        "id": "A97LOui_Sbmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image dimensions\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "path = os.path.join(ROOT, 'test/color/000.png')\n",
        "image = Image.open(path)\n",
        "image"
      ],
      "metadata": {
        "id": "6IPr1uFdUi6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The image dimension are: {np.array(image).shape}')"
      ],
      "metadata": {
        "id": "Ex2BWCI7WU2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2\n",
        "Implement 4 different attempts to improve the model's performance trained on the dataset you choose in previous question. Ensure that at least one approach involves modifying the pre-trained model from TorchVision. Summarize the outcomes of each attempt, highlighting the best performing model and the key factors contributing to its success. You may also need to describe other hyperparameters you use in your experiment, like epochs, learning rate, and optimizer."
      ],
      "metadata": {
        "id": "RhfoeIvTdOzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
        "\n"
      ],
      "metadata": {
        "id": "OzBWq5Vy9Wdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!TORCH_USE_CUDA_DSA=1"
      ],
      "metadata": {
        "id": "mY9cWiCWe1o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline approach\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, 6)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.convs(x)\n",
        "\n",
        "\n",
        "def freeze_weights(module):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = False\n",
        "    return None\n",
        "\n",
        "\n",
        "def unfreeze_weights(module):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = True\n",
        "    return None\n",
        "\n",
        "\n",
        "# Torchvision models\n",
        "class TorchvisionResNet18(nn.Module):\n",
        "    def __init__(self, num_classes: int = 6, is_pretrained: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.resnet18(pretrained=is_pretrained)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        freeze_weights(self.model)\n",
        "        unfreeze_weights(self.model.fc)\n",
        "\n",
        "    def unfreeze_model(self):\n",
        "        unfreeze_weights(self.model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class TorchVisionEfficientNetb0(nn.Module):\n",
        "    def __init__(self, num_classes: int = 6, is_pretrained: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.model = torchvision.models.efficientnet_b0(pretrained=is_pretrained)\n",
        "        self.model.classifier = nn.Linear(self.model.classifier[-1].in_features, num_classes)\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        freeze_weights(self.model)\n",
        "        unfreeze_weights(self.model.classifier)\n",
        "\n",
        "    def unfreeze_model(self):\n",
        "        unfreeze_weights(self.model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "# training utils\n",
        "def create_transforms(input_size: int = 224):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.AutoAugment(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, test_transform\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_paths: list,\n",
        "        class_to_index: ty.Dict = None,\n",
        "        transform: ty.Callable = None\n",
        "    ) -> None:\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        self.class_to_index = class_to_index if class_to_index is not None \\\n",
        "            else self._get_class_index()\n",
        "\n",
        "    def _get_class_index(self):\n",
        "        self.classes = sorted(set(path.split('/')[-2] for path in self.image_paths))\n",
        "        return {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = img_path.split('/')[-2]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, self.class_to_index[label]\n",
        "\n",
        "\n",
        "def create_data_loader(\n",
        "        train_image_paths: list,\n",
        "        test_image_paths: list,\n",
        "        train_transform: ty.Callable = None,\n",
        "        test_transform: ty.Callable = None,\n",
        "        batch_size: int = 32) -> ty.Tuple[DataLoader]:\n",
        "\n",
        "    train_dataset = CustomImageDataset(train_image_paths, None, train_transform)\n",
        "    test_dataset = CustomImageDataset(test_image_paths, train_dataset.class_to_index, test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, lr_scheduler, loader):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for features, labels in loader:\n",
        "        features = features.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = model(features)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted = outputs.argmax(-1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total_samples\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "@torch.no_grad\n",
        "def evaluate(model, criterion, loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for features, labels in loader:\n",
        "        features = features.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = model(features)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        predicted = outputs.argmax(-1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total_samples\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def run_experiment(train_loader, test_loader, model, optimizer_class, learning_rate, num_epochs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
        "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_train_loss, train_accuracy = train_one_epoch(\n",
        "            model, criterion, optimizer, lr_scheduler, train_loader\n",
        "        )\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "    avg_test_loss, test_accuracy = evaluate(model, criterion, test_loader)\n",
        "\n",
        "    return train_losses, train_accuracies, avg_test_loss, test_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "WcdWiO6bWYCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    train_transform, test_transform = create_transforms()\n",
        "    train_loader, test_loader = create_data_loader(\n",
        "        train_image_paths, test_image_paths, train_transform, test_transform, batch_size=32\n",
        "    )\n",
        "\n",
        "    # Experiment 1: Simple CNN (1)\n",
        "    model = SimpleCNN().to(DEVICE)\n",
        "    num_epochs = 30\n",
        "    optimizer_class = optim.SGD\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    print('=============== Experiment 1 ===============')\n",
        "    print(f'model: SimpleCNN')\n",
        "    print(f'Number of epoch: {num_epochs}')\n",
        "    print(f'Optimizer: Adam')\n",
        "    print(f'Learning rate: {learning_rate}')\n",
        "\n",
        "    train_losses, train_accuracies, avg_test_loss, test_accuracy = run_experiment(\n",
        "        train_loader, test_loader, model, optimizer_class, learning_rate, num_epochs\n",
        "    )\n",
        "\n",
        "    print('--------------- Performance ---------------')\n",
        "    print(f'Training loss: {train_losses[-1]}')\n",
        "    print(f'Training acc: {train_accuracies[-1]}')\n",
        "\n",
        "    print(f'Testing loss: {avg_test_loss}')\n",
        "    print(f'Testing acc: {test_accuracy}')\n",
        "\n",
        "    del model\n",
        "\n",
        "    # Experiment 2: Simple CNN (2)\n",
        "    model = SimpleCNN().to(DEVICE)\n",
        "    num_epochs = 30\n",
        "    optimizer_class = optim.Adam\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    print('=============== Experiment 2 ===============')\n",
        "    print(f'model: SimpleCNN')\n",
        "    print(f'Number of epoch: {num_epochs}')\n",
        "    print(f'Optimizer: Adam')\n",
        "    print(f'Learning rate: {learning_rate}')\n",
        "\n",
        "    train_losses, train_accuracies, avg_test_loss, test_accuracy = run_experiment(\n",
        "        train_loader, test_loader, model, optimizer_class, learning_rate, num_epochs\n",
        "    )\n",
        "\n",
        "    print('--------------- Performance ---------------')\n",
        "    print(f'Training loss: {train_losses[-1]}')\n",
        "    print(f'Training acc: {train_accuracies[-1]}')\n",
        "\n",
        "    print(f'Testing loss: {avg_test_loss}')\n",
        "    print(f'Testing acc: {test_accuracy}')\n",
        "\n",
        "    del model\n",
        "\n",
        "    # Experiment 3: ResNet18 (train from srach)\n",
        "    model = TorchvisionResNet18(is_pretrained=False).to(DEVICE)\n",
        "    num_epochs = 30\n",
        "    optimizer_class = optim.Adam\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    print('=============== Experiment 3 ===============')\n",
        "    print(f'model: ResNet 18 (train from scratch)')\n",
        "    print(f'Number of epoch: {num_epochs}')\n",
        "    print(f'Optimizer: Adam')\n",
        "    print(f'Learning rate: {learning_rate}')\n",
        "\n",
        "    train_losses, train_accuracies, avg_test_loss, test_accuracy = run_experiment(\n",
        "        train_loader, test_loader, model, optimizer_class, learning_rate, num_epochs\n",
        "    )\n",
        "\n",
        "    print('--------------- Performance ---------------')\n",
        "    print(f'Training loss: {train_losses[-1]}')\n",
        "    print(f'Training acc: {train_accuracies[-1]}')\n",
        "\n",
        "    print(f'Testing loss: {avg_test_loss}')\n",
        "    print(f'Testing acc: {test_accuracy}')\n",
        "\n",
        "    del model\n",
        "\n",
        "    # Experiment 4: ResNet18 (transfer learning)\n",
        "    model = TorchvisionResNet18(is_pretrained=True).to(DEVICE)\n",
        "    model.freeze_feature_extractor()\n",
        "    num_epochs = 30\n",
        "    optimizer_class = optim.Adam\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    print('=============== Experiment 4 ===============')\n",
        "    print(f'model: ResNet 18 (transfer learning)')\n",
        "    print(f'Number of epoch: {num_epochs}')\n",
        "    print(f'Optimizer: Adam')\n",
        "    print(f'Learning rate: {learning_rate}')\n",
        "\n",
        "    train_losses, train_accuracies, avg_test_loss, test_accuracy = run_experiment(\n",
        "        train_loader, test_loader, model, optimizer_class, learning_rate, num_epochs\n",
        "    )\n",
        "\n",
        "    print('--------------- Performance ---------------')\n",
        "    print(f'Training loss: {train_losses[-1]}')\n",
        "    print(f'Training acc: {train_accuracies[-1]}')\n",
        "\n",
        "    print(f'Testing loss: {avg_test_loss}')\n",
        "    print(f'Testing acc: {test_accuracy}')\n",
        "\n",
        "    # Experiment 5: ResNet18 (transfer learning + fine-tuning)\n",
        "    model.unfreeze_model()\n",
        "    num_epochs = 10\n",
        "    optimizer_class = optim.Adam\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    print('=============== Experiment 5 ===============')\n",
        "    print(f'model: ResNet 18 (fine-tuning)')\n",
        "    print(f'Number of epoch: {num_epochs}')\n",
        "    print(f'Optimizer: Adam')\n",
        "    print(f'Learning rate: {learning_rate}')\n",
        "\n",
        "    train_losses, train_accuracies, avg_test_loss, test_accuracy = run_experiment(\n",
        "        train_loader, test_loader, model, optimizer_class, learning_rate, num_epochs\n",
        "    )\n",
        "\n",
        "    print('--------------- Performance ---------------')\n",
        "    print(f'Training loss: {train_losses[-1]}')\n",
        "    print(f'Training acc: {train_accuracies[-1]}')\n",
        "\n",
        "    print(f'Testing loss: {avg_test_loss}')\n",
        "    print(f'Testing acc: {test_accuracy}')\n",
        "\n",
        "    del model"
      ],
      "metadata": {
        "id": "GPYM_mVtFxhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "sotzzkmMVQVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jyhcSBpFVR3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}